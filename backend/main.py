import os
from typing import Dict
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_core.messages import SystemMessage
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from pdf_importer import create_vector_store, CONNECTION_STRING, COLLECTION_NAME

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# RAG êµ¬ì„± ìš”ì†Œë¥¼ í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
embeddings = HuggingFaceEmbeddings(
    model_name='nlpai-lab/KURE-v1',
    model_kwargs={'device': 'cpu'}
)

try:
    vector_store = PGVector(
        collection_name=COLLECTION_NAME,
        connection_string=CONNECTION_STRING,
        embedding_function=embeddings
    )
    print("Vector store loaded from PostgreSQL.")
except Exception as e:
    print(f"Error connecting to PostgreSQL: {e}")
    print("Creating a new vector store...")
    vector_store = create_vector_store()
    if vector_store is None:
        exit("Error: Vector store could not be created.")

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    model_kwargs={
        "system_instruction": SystemMessage(content="""ë‹¹ì‹ ì€ ëª…ì§€ì „ë¬¸ëŒ€í•™ í•™ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ë‹µë³€ ì›ì¹™: 1. ëª…ì§€ì „ë¬¸ëŒ€í•™ ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•©ë‹ˆë‹¤. 
        2. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•˜ê³  ìœ ì—°í•˜ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤. 
        3. ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ë©°, ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. 
        4. ì°¸ê³  ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì„ì˜ë¡œ ë‹µë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
        ë‹µë³€ ê·œì¹™: - ëª…ì§€ì „ë¬¸ëŒ€í•™ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸: "ì£„ì†¡í•©ë‹ˆë‹¤. ëª…ì§€ì „ë¬¸ëŒ€í•™ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
        - ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ì°¸ê³  ë¬¸ì„œì— ëª…í™•í•˜ê²Œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ì–´ë–¤ ë‚´ìš©ë„ ì¶”ë¡ í•˜ê±°ë‚˜ ë§ë¶™ì´ì§€ ë§ê³  ë¬´ì¡°ê±´ "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.""")
    }
)

retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 3,
        "fetch_k": 5,  # MMRì—ì„œ ê³ ë ¤í•  í›„ë³´ ë¬¸ì„œ ìˆ˜
        "lambda_mult": 0.7,  # ë‹¤ì–‘ì„± vs ê´€ë ¨ì„± ê°€ì¤‘ì¹˜ (0.5ëŠ” ê· í˜•)
        "score_threshold": 0.7  # ì ìˆ˜ ì„ê³„ê°’ (0.7ì€ ë†’ì€ ì •í™•ë„ë¥¼ ìœ„í•œ ê¸°ì¤€)
    }
)

# ì‚¬ìš©ì ì„¸ì…˜ë³„ ëŒ€í™” ì²´ì¸ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
chat_sessions: Dict[str, ConversationalRetrievalChain] = {}

def get_or_create_chain(session_id: str) -> ConversationalRetrievalChain:
    if session_id not in chat_sessions:
        memory = ConversationBufferMemory(
            memory_key="chat_history", 
            return_messages=True,
            output_key="answer"  # ì´ ë¶€ë¶„ì´ í•„ìˆ˜!
        )
        new_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True  # ì†ŒìŠ¤ ë¬¸ì„œ ë°˜í™˜ í™œì„±í™”
        )
        chat_sessions[session_id] = new_chain
        print(f"ìƒˆë¡œìš´ ì„¸ì…˜ ID ìƒì„±: {session_id}")
    return chat_sessions[session_id]

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatMessage(BaseModel):
    message: str
    session_id: str

class ChatResponse(BaseModel):
    response: str
    success: bool
    
@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_gemini(request: ChatMessage):
    try:
        qa_chain = get_or_create_chain(request.session_id)
        result = qa_chain.invoke({"question": request.message})
        
        # MMRë¡œ ë½‘íŒ ë¬¸ì„œë“¤ ë¡œê·¸ ì¶œë ¥
        if 'source_documents' in result:
            print(f"\n=== MMR ê²€ìƒ‰ ê²°ê³¼ (ì§ˆë¬¸: {request.message}) ===")
            print(f"ì´ {len(result['source_documents'])}ê°œ ë¬¸ì„œ ì„ íƒë¨")
            for i, doc in enumerate(result['source_documents'], 1):
                print(f"\nğŸ“„ ë¬¸ì„œ {i}:")
                print(f"   ë‚´ìš©: {doc.page_content[:200]}...")  # ì²˜ìŒ 200ìë§Œ ì¶œë ¥
                if hasattr(doc, 'metadata'):
                    print(f"   ë©”íƒ€ë°ì´í„°: {doc.metadata}")
                print("-" * 50)
        else:
            print("âš ï¸ ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        return ChatResponse(
            response=result['answer'],
            success=True
        )
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return ChatResponse(
            response=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", 
            success=False
        )

@app.get("/")
async def root():
    return {"message": "ëª…ì§€ì „ë¬¸ëŒ€í•™ í•™ì‚¬ ì±—ë´‡ API"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)